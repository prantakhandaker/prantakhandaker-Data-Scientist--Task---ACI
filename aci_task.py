# -*- coding: utf-8 -*-
"""ACI TASK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b5O1zDLAiOoy42gDllkO1uiozVpx5K9D

##SECTION A --  Tractor Credit Risk Analysis  (Q1)
"""

# STEP 1: Install pandasql
!pip install -q pandasql

# STEP 2: Import libraries
import pandas as pd
from pandasql import sqldf

# STEP 3: Load the CSV
df = pd.read_csv('/content/tractor_credit_data.csv')

pysqldf = lambda q: sqldf(q, globals())

query = """
SELECT
    upazila,
    COUNT(CASE WHEN emi_paid_months < 0.5 * total_emi_months THEN 1 END) * 1.0 / COUNT(*) AS low_emi_ratio,
    COUNT(*) AS total_customers
FROM
    df
GROUP BY
    upazila
HAVING
    COUNT(CASE WHEN emi_paid_months < 0.5 * total_emi_months THEN 1 END) > 0
ORDER BY
    low_emi_ratio DESC
LIMIT 5;
"""


top_5_upazilas = pysqldf(query)

top_5_upazilas

"""##SECTION A --  Tractor Credit Risk Analysis  (Q2)"""

import pandas as pd
import numpy as np
!pip install -q pandasql
from pandasql import sqldf
import nltk
import string
import re
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.cluster import KMeans
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np

# Load the dataset
df = pd.read_csv("/content/tractor_credit_data.csv")

df

# Step 1: Calculate EMI Paid Ratio
df["emi_paid_ratio"] = df["emi_paid_months"] / df["total_emi_months"]

df

def calculate_risk_score(row):
    score = 0

    # EMI Paid Ratio
    if row["emi_paid_ratio"] < 0.5:
        score += 2
    elif row["emi_paid_ratio"] < 0.75:
        score += 1

    # Payment Delay Days
    if row["payment_delay_days"] > 30:
        score += 2
    elif row["payment_delay_days"] > 0:
        score += 1

    # Income Doc Score
    if row["income_doc_score"] < 400:
        score += 2
    elif row["income_doc_score"] < 600:
        score += 1

    return score

# Apply scoring
df["risk_score"] = df.apply(calculate_risk_score, axis=1)

df

# Step 3: Categorize risk segment
def risk_category(score):
    if score <= 1:
        return "Low"
    elif score <= 3:
        return "Medium"
    else:
        return "High"

df["risk_segment"] = df["risk_score"].apply(risk_category)

df

segment_counts = df["risk_segment"].value_counts().reset_index()
segment_counts.columns = ["risk_segment", "customer_count"]

segment_counts

avg_delay_per_segment = df.groupby("risk_segment")["payment_delay_days"].mean().reset_index()
avg_delay_per_segment.columns = ["risk_segment", "avg_payment_delay_days"]

avg_delay_per_segment

risk_summary = pd.merge(segment_counts, avg_delay_per_segment, on="risk_segment")

risk_summary

"""##SECTION B — Fraud Pattern in Dealership Transactions (Q3)"""

pysqldf = lambda q: sqldf(q, globals())

query = """
SELECT
    dealer_id,
    SUM(CASE WHEN customer_nid_duplicate_flag = 1 OR same_phone_multiple_buyers_flag = 1 THEN 1 ELSE 0 END) AS fraud_count,
    COUNT(*) AS total_transactions
FROM df
GROUP BY dealer_id
HAVING SUM(CASE WHEN customer_nid_duplicate_flag = 1 OR same_phone_multiple_buyers_flag = 1 THEN 1 ELSE 0 END) > 3
ORDER BY fraud_count DESC;
"""

result = pysqldf(query)

print(result)

"""##SECTION B — Fraud Pattern in Dealership Transactions (Q4)"""

# Step 1: Load the CSV data
df = pd.read_csv('/content/dealer_transaction_data.csv')

df

# Calculate suspicious activity rate
df['is_suspicious'] = (df['customer_nid_duplicate_flag'] == 1) | (df['same_phone_multiple_buyers_flag'] == 1)

df

dealer_stats = df.groupby('dealer_id').agg(
    total_transactions=('transaction_id', 'count'),
    suspicious_transactions=('is_suspicious', 'sum')
).reset_index()

dealer_stats

dealer_stats['suspicious_rate'] = (dealer_stats['suspicious_transactions'] / dealer_stats['total_transactions']) * 100

# Select top 10 dealers by suspicious activity rate
top_10_dealers = dealer_stats.sort_values(by='suspicious_rate', ascending=False).head(10)

top_10_dealers

# Create a horizontal bar chart
plt.figure(figsize=(10, 6))
plt.barh(top_10_dealers['dealer_id'], top_10_dealers['suspicious_rate'], color='#3b82f6')
plt.xlabel('Suspicious Activity Rate (%)')
plt.ylabel('Dealer ID')
plt.title('Top 10 Dealers by Suspicious Activity Rate')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest rate at the top
plt.grid(True, axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()

plt.savefig('top_10_dealers_suspicious_activity_rate.png')
plt.show()

# Analyze and print observations
print(
    f"Summary:\n"
    f"- Suspicious activity rate reflects the % of transactions flagged by NID or phone duplication.\n"
    f"- Top dealer ({top_10_dealers.iloc[0]['dealer_id']}) has a high suspicious rate of {top_10_dealers.iloc[0]['suspicious_rate']:.2f}%.\n"
    f"- {'Multiple dealers show rates >50%, indicating potential fraud clusters.' if top_10_dealers['suspicious_rate'].max() > 50 else 'No dealer exceeds 50% suspicious rate.'}\n"
    f"- {'Several dealers fall in the 40–50% range, suggesting similar verification issues.' if (top_10_dealers['suspicious_rate'].between(40, 50).sum() > 3) else 'No strong clustering between 40–50% range.'}\n"
    f"- High-risk dealers should be investigated further."
)

"""##SECTION C – Location-Based Segmentation & Marketing (Q5)

"""

# Load the dataset
data = pd.read_csv('/content/yamaha_customer_data_with_geo.csv')

data

# numeric columns
numeric_cols = ['monthly_income', 'purchase_intent_score', 'visit_frequency', 'latitude', 'longitude']
data[numeric_cols] = data[numeric_cols].apply(pd.to_numeric, errors='coerce')

data[numeric_cols]

data = data.dropna(subset=numeric_cols)

data

#Customer Segmentation
features = ['monthly_income', 'purchase_intent_score', 'visit_frequency']
X = data[features]

X

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

# Apply K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
data['cluster'] = kmeans.fit_predict(X_scaled)

data['cluster']

cluster_stats = data.groupby('cluster').agg({
    'monthly_income': 'mean',
    'purchase_intent_score': 'mean',
    'visit_frequency': 'mean'
}).reset_index()

cluster_stats

def assign_cluster_name(row):
    if row['monthly_income'] > 75000 and row['purchase_intent_score'] > 0.7:
        return 'Premium Enthusiasts'
    elif row['visit_frequency'] > 6:
        return 'Frequent Browsers'
    else:
        return 'Budget Commuters'

cluster_stats['cluster_name'] = cluster_stats.apply(assign_cluster_name, axis=1)
cluster_name_map = cluster_stats.set_index('cluster')['cluster_name'].to_dict()
data['cluster_name'] = data['cluster'].map(cluster_name_map)

data['cluster_name']

fig1 = px.scatter(
    data,
    x='monthly_income',
    y='purchase_intent_score',
    color='cluster_name',
    title='Customer Segments: Income vs Purchase Intent',
    labels={
        'monthly_income': 'Monthly Income (BDT)',
        'purchase_intent_score': 'Purchase Intent Score',
        'cluster_name': 'Customer Segment'
    },
    color_discrete_map={
        'Premium Enthusiasts': '#ff7300',
        'Frequent Browsers': '#00C49F',
        'Budget Commuters': '#8884d8'
    }
)
# fig1.update_layout(showlegend=True)

fig1

# Plot 2: Customer density heatmap
fig2 = go.Figure(go.Scattergeo(
    lon=data['longitude'],
    lat=data['latitude'],
    mode='markers',
    marker=dict(
        size=8,
        opacity=0.3,
        color='#ff7300'
    ),
    text=data['upazila']
))
fig2.update_layout(
    title='Customer Density by Location',
    geo=dict(
        scope='world',
        projection_type='mercator',
        center=dict(lat=23.7, lon=90.0),
        lataxis_range=[22, 25],
        lonaxis_range=[88, 91],
        showland=True,
        landcolor='rgb(243, 243, 243)',
        countrycolor='rgb(204, 204, 204)'
    )
)

# High-Potential Upazilas

filtered_data = data[data['product_type'].isin(['Sports', 'Scooter'])]

# Group by upazila and calculate metrics
upazila_stats = filtered_data.groupby('upazila').agg({
    'monthly_income': 'mean',
    'purchase_intent_score': 'mean',
    'customer_id': 'count',
    'product_type': lambda x: list(set(x))
}).reset_index()

upazila_stats

sports_data = data[data['product_type'] == 'Sports']
upazila_sports_counts = sports_data.groupby('upazila').agg({
    'customer_id': 'count',
    'monthly_income': 'mean',
    'purchase_intent_score': 'mean'
}).reset_index()
upazila_sports_counts = upazila_sports_counts.sort_values(by='purchase_intent_score', ascending=False)

upazila_sports_counts

for _, row in upazila_sports_counts.head(3).iterrows():
    print(f"- {row['upazila']}: {row['customer_id']} customers, Avg Income: {row['monthly_income']:.0f} BDT, "
          f"Avg Intent: {row['purchase_intent_score']:.2f}")

"""##SECTION C – Location-Based Segmentation & Marketing (Q6)"""

upazila_stats.columns = ['upazila', 'avg_income', 'avg_intent', 'customer_count', 'product_types']

# Filter upazilas based on criteria
high_potential = upazila_stats[
    (upazila_stats['avg_income'] > 50000) &
    (upazila_stats['avg_intent'] > 0.6)
]

upazila_stats.columns

top_upazilas = high_potential.sort_values(by='avg_intent', ascending=False).head(3)

top_upazilas

# Format the output
top_upazilas['avg_income'] = top_upazilas['avg_income'].round(0).astype(int)
top_upazilas['avg_intent'] = top_upazilas['avg_intent'].round(2)
top_upazilas['product_types'] = top_upazilas['product_types'].apply(lambda x: ', '.join(x))

top_upazilas[['upazila', 'customer_count', 'avg_income', 'product_types']].to_string(index=False)

print("\nTop 3 High-Potential Upazilas:")
print(top_upazilas[['upazila', 'customer_count', 'avg_income', 'product_types']].to_string(index=False))

top_upazilas.to_csv('top_upazilas.csv', index=False)

for upazila in top_upazilas['upazila']:
    print(f"- {upazila}: High income and intent, with interest in {top_upazilas[top_upazilas['upazila'] == upazila]['product_types'].iloc[0]}")

"""##SECTION D — Financial Monitoring & Insight Dashboard (Q7)

##1
"""

df = pd.read_csv('/content/financial_tracking_data.csv')

df

#handle missing/invalid values
numeric_cols = ['total_units_sold', 'credit_units', 'cash_units', 'total_revenue',
                'total_discount', 'avg_emi_delay_days', 'operational_cost',
                'net_profit', 'marketing_spend', 'roi']
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

df

# format large numbers for display
def format_number(num):
    if num >= 1e6:
        return f"{num / 1e6:.1f}M"
    elif num >= 1e3:
        return f"{num / 1e3:.1f}K"
    return f"{num:.0f}"

fig = px.bar(
        regional_roi,
        x='region',
        y='roi',
        title='Average ROI by Region',
        labels={'roi': 'Average ROI', 'region': 'Region'},
        color='roi',
        color_continuous_scale=['#ef4444' if x < 200 else '#3b82f6' for x in regional_roi['roi']]
    )
fig.update_layout(
        xaxis_title="Region",
        yaxis_title="Average ROI",
        xaxis_tickangle=-45,
        showlegend=False,
        height=500,
        template='plotly_white'
    )
fig.show()

regional_roi

"""### Task 1: Regional ROI Analysis"""

fig = px.bar(
        regional_roi,
        x='region',
        y='roi',
        title='Average ROI by Region',
        labels={'roi': 'Average ROI', 'region': 'Region'},
        color='roi',
        color_continuous_scale=['#ef4444' if x < 200 else '#3b82f6' for x in regional_roi['roi']]
    )
fig.update_layout(
        xaxis_title="Region",
        yaxis_title="Average ROI",
        xaxis_tickangle=-45,
        showlegend=False,
        height=500,
        template='plotly_white'
    )

print(f"Regional ROI Insights: {regional_roi.iloc[0]['region']} leads with highest ROI ({regional_roi.iloc[0]['roi']:.2f}); recommend auditing low-ROI regions and reallocating marketing or training dealers.")

"""## Task 2: Profit vs Marketing Spend Analysis"""

scatter_data = df[['marketing_spend', 'net_profit']].copy()

scatter_data

# Calculate linear regression
slope, intercept = np.polyfit(scatter_data['marketing_spend'], scatter_data['net_profit'], 1)
trendline_x = np.array([scatter_data['marketing_spend'].min(), scatter_data['marketing_spend'].max()])
trendline_y = slope * trendline_x + intercept

trendline_x

# Calculate correlation coefficient
correlation = scatter_data.corr().iloc[0, 1]
correlation

# Create scatter plot with trendline
fig = px.scatter(
        scatter_data,
        x='marketing_spend',
        y='net_profit',
        title='Net Profit vs Marketing Spend',
        labels={'marketing_spend': 'Marketing Spend', 'net_profit': 'Net Profit'},
        color_discrete_sequence=['#3b82f6']
    )
fig.add_trace(
        go.Scatter(
            x=trendline_x,
            y=trendline_y,
            mode='lines',
            name='Trendline',
            line=dict(color='#ef4444')
        )
    )
fig.update_layout(
        xaxis_title="Marketing Spend",
        yaxis_title="Net Profit",
        showlegend=True,
        height=500,
        template='plotly_white',
        xaxis_tickformat='~s',
        yaxis_tickformat='~s'
    )

    # Save or display the plot
fig.show()

print(f"Profit vs Marketing Spend: Corr={correlation:.2f}, showing a moderate positive link. Some dealers earn >15M profit with <20K spend, highlighting strong efficiency.")

"""##SECTION E — Analyze and Visualize Customer Feedback (Q8)

#Processing
"""

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load data
df = pd.read_csv('/content/yamaha_mock_customer_feedback.csv')
df

def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    tokens = word_tokenize(text)  # Tokenize
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize
    return ' '.join(tokens)

df['cleaned_feedback'] = df['comment_text'].apply(preprocess_text)

df

"""##Sentiment Analysis"""

sia = SentimentIntensityAnalyzer()

def get_sentiment(text):
    sentiment = sia.polarity_scores(text)
    if sentiment['compound'] >= 0.05:
        return 'Positive'
    elif sentiment['compound'] <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment'] = df['cleaned_feedback'].apply(get_sentiment)

df

# Plot overall sentiment distribution
sentiment_counts = df['sentiment'].value_counts()
plt.figure(figsize=(8, 6))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')
plt.title('Overall Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Comments')
plt.savefig('overall_sentiment_distribution.png')
plt.show()

# Show sentiment by product category
sentiment_by_category = df.groupby('product_category')['sentiment'].value_counts(normalize=True).unstack().fillna(0)
sentiment_by_category.plot(kind='bar', stacked=True, figsize=(12, 7), cmap='coolwarm')
plt.title('Sentiment Distribution by Product Category')
plt.xlabel('Product Category')
plt.ylabel('Proportion of Comments')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('sentiment_by_product_category.png')
plt.show()

"""##Topic Modeling"""

vectorizer = TfidfVectorizer(max_df=0.85, min_df=2, stop_words='english')
X = vectorizer.fit_transform(df['cleaned_feedback'])

num_topics = 4  # Aim for 3-5 themes
kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)
kmeans.fit(X)
df['topic'] = kmeans.labels_

df

print("\nTop 5 keywords per topic:")
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names_out()
for i in range(num_topics):
    print(f"Topic {i}:")
    for ind in order_centroids[i, :5]:
        print(f' - {terms[ind]}')

"""##Word Cloud"""

positive_feedback = ' '.join(df[df['sentiment'] == 'Positive']['cleaned_feedback'])
negative_feedback = ' '.join(df[df['sentiment'] == 'Negative']['cleaned_feedback'])

positive_feedback

wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_feedback)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Positive Reviews')
plt.savefig('wordcloud_positive.png')
plt.show()

wordcloud_negative = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(negative_feedback)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Negative Reviews')
plt.savefig('wordcloud_negative.png')
plt.show()

print("""
**Summary for ACI Motors’ Marketing Team**

3 Key Insights:

1. Sentiment Varies by Product Line
   Customer sentiment differs across Yamaha products—some are praised for performance, others face recurring issues. This signals the need for customized marketing per product line.

2. After-Sales Service is a Major Pain Point
   Many customers express frustration over poor service and unresponsive after-sales support, which damages brand reputation and loyalty.

3. Performance Gaps Fuel Complaints
   Frequent mentions of engine problems and mileage not meeting expectations suggest a disconnect between marketing promises and real-world product performance.

Recommended Action:

Launch a “Service Excellence” Campaign
Invest in dealer training, set service quality standards, and highlight improved support in your messaging to rebuild trust and improve satisfaction.
""")